1. Fetching data from the YouTube API into Database 

```{python}

import pandas as pd
import datetime
from sqlalchemy import create_engine
from googleapiclient.discovery import build
from google_auth_oauthlib.flow import InstalledAppFlow

# Set up YouTube API connection
API_KEY = "AIzaSyCW70WzNcZYDOz2-y8yJa7dAJgDke9kCqM"
OAUTH_CREDENTIALS = "client_secret_29238299626-95koqi0a53ageu7d25tuikquappn3l22.apps.googleusercontent.com.json"

# Initialize the YouTube Data API client
youtube = build("youtube", "v3", developerKey=API_KEY)
print("YouTube Data API connected successfully!")

SCOPES = [
    "https://www.googleapis.com/auth/yt-analytics.readonly",
    "https://www.googleapis.com/auth/yt-analytics-monetary.readonly",
    "https://www.googleapis.com/auth/youtube.readonly"
]

flow = InstalledAppFlow.from_client_secrets_file(OAUTH_CREDENTIALS, SCOPES)
credentials = flow.run_local_server(port=0)
youtube_analytics = build("youtubeAnalytics", "v2", credentials=credentials)
print("YouTube Analytics API connected successfully!")

# Function to fetch analytics data with optional filters
def fetch_analytics_data(start_date, end_date, metrics, dimensions, sort, filters=None):
    data = []
    request = youtube_analytics.reports().query(
        ids="channel==MINE",
        startDate=start_date,
        endDate=end_date,
        metrics=metrics,
        dimensions=dimensions,
        sort=sort,
        filters=filters
    )
    response = request.execute()

    # Extract column headers as names
    column_headers = [header["name"] for header in response["columnHeaders"]]

    for row in response.get("rows", []):
        entry = dict(zip(column_headers, row))
        data.append(entry)
    
    return pd.DataFrame(data)

# Function to fetch video comments
def fetch_video_comments(video_id):
    comments = []
    request = youtube.commentThreads().list(
        part="snippet",
        videoId=video_id,
        textFormat="plainText"
    )
    
    response = request.execute()
    
    # Collect comments from the response
    while response:
        for item in response["items"]:
            comment = item["snippet"]["topLevelComment"]["snippet"]["textDisplay"]
            comments.append(comment)
        
        # Check if there are more pages of comments
        if "nextPageToken" in response:
            request = youtube.commentThreads().list(
                part="snippet",
                videoId=video_id,
                pageToken=response["nextPageToken"],
                textFormat="plainText"
            )
            response = request.execute()
        else:
            break
    
    return comments

# Function to get video IDs from the channel
def get_video_ids(channel_id):
    video_ids = []
    request = youtube.search().list(
        part="snippet",
        channelId=channel_id,
        maxResults=50,  # Adjust the number as needed
        type="video"
    )
    
    response = request.execute()
    
    for item in response["items"]:
        video_ids.append(item["id"]["videoId"])
    
    return video_ids

# Set up SQLAlchemy connection to PostgreSQL
engine = create_engine("postgresql://postgres:1999%40Johannes@localhost:5432/youtube_data")

# Fetch and insert all datasets into PostgreSQL
def insert_data_from_api():
    # Fetch daily video metrics from YouTube API
    start_date = "2019-01-01"  # Adjust as needed
    end_date = datetime.datetime.now().strftime("%Y-%m-%d")

    # Fetch daily video metrics
    daily_video_df = fetch_analytics_data(
        start_date=start_date,
        end_date=end_date,
        metrics="views,estimatedMinutesWatched,averageViewDuration,averageViewPercentage,subscribersGained",
        dimensions="day",
        sort="day"
    )

    # Insert data into PostgreSQL
    daily_video_df.to_sql('daily_video_metrics', engine, if_exists='replace', index=False)
    print("daily_video_metrics data inserted into PostgreSQL.")

    # Fetch comments from each video
    channel_id = "UCiosg1akiDnp4fq513TOmmw"
    video_ids = get_video_ids(channel_id)
    
    all_comments = []
    for video_id in video_ids:
        comments = fetch_video_comments(video_id)
        for comment in comments:
            all_comments.append({"video_id": video_id, "comment": comment})
    
    # Create a DataFrame for comments
    comments_df = pd.DataFrame(all_comments)

    # Insert comments into PostgreSQL
    comments_df.to_sql('comments', engine, if_exists='replace', index=False)
    print("Comments data inserted into PostgreSQL.")

    # Fetch other metrics and insert them into PostgreSQL (e.g., daily annotation metrics, traffic source, etc.)
    # You can repeat the process for the other queries like `daily_annotation_metrics`, `traffic_source_metrics`, etc.
    for query in queries:
        print(f"Fetching data for {query['name']}...")
        df = fetch_analytics_data(
            start_date=start_date,
            end_date=end_date,
            metrics=query["metrics"],
            dimensions=query["dimensions"],
            sort=query["sort"],
            filters=query.get("filters")  # Pass filters if present
        )
        # Insert data into the database
        df.to_sql(query["name"], engine, if_exists='replace', index=False)
        print(f"{query['name']} data inserted into PostgreSQL.")

# Define the queries based on the documentation (metrics, dimensions, etc.)
queries = [
    {
        "name": "daily_annotation_metrics",
        "metrics": "views,likes,annotationClickThroughRate,annotationCloseRate,annotationImpressions",
        "dimensions": "day",
        "sort": "day"
    },
    {
        "name": "daily_country_specific_metrics",
        "metrics": "views,estimatedMinutesWatched,averageViewDuration,averageViewPercentage,subscribersGained",
        "dimensions": "country",
        "sort": "-estimatedMinutesWatched"
    },
    {
        "name": "traffic_source_metrics",
        "metrics": "views,estimatedMinutesWatched",
        "dimensions": "day,insightTrafficSourceType",
        "sort": "day"
    },
    {
        "name": "revenue_metrics",
        "metrics": "views,estimatedRevenue,estimatedAdRevenue,estimatedRedPartnerRevenue,grossRevenue,adImpressions,cpm,playbackBasedCpm,monetizedPlaybacks",
        "dimensions": "day",
        "sort": "day"
    },
    {
        "name": "ad_type_metrics",
        "metrics": "grossRevenue,adImpressions,cpm",
        "dimensions": "adType",
        "sort": "-adType"
    },
    {
        "name": "sharing_metrics",
        "metrics": "shares",
        "dimensions": "sharingService",
        "sort": "-shares"
    },
    {
        "name": "province_metrics",
        "metrics": "views,estimatedMinutesWatched,averageViewDuration",
        "dimensions": "province",
        "filters": "country==US", 
        "sort": "province"
    }
]

# Run the function to insert data from the API
insert_data_from_api()

```


## 2.Loading Data from PostgreSQL

```{python}
import pandas as pd
from sqlalchemy import create_engine

# Set up SQLAlchemy connection to PostgreSQL
engine = create_engine("postgresql://postgres:1999%40Johannes@localhost:5432/youtube_data")

# Load data from all tables
daily_video_metrics = pd.read_sql('SELECT * FROM daily_video_metrics', engine)
comments = pd.read_sql('SELECT * FROM comments', engine)
province_metrics = pd.read_sql('SELECT * FROM province_metrics', engine)
daily_annotation_metrics = pd.read_sql('SELECT * FROM daily_annotation_metrics', engine)
traffic_source_metrics = pd.read_sql('SELECT * FROM traffic_source_metrics', engine)
revenue_metrics = pd.read_sql('SELECT * FROM revenue_metrics', engine)
ad_type_metrics = pd.read_sql('SELECT * FROM ad_type_metrics', engine)
sharing_metrics = pd.read_sql('SELECT * FROM sharing_metrics', engine)
daily_country_specific_metrics = pd.read_sql('SELECT * FROM daily_country_specific_metrics', engine)




```

```{python}

# Calculate daily growth rate in views
daily_video_metrics['views_growth'] = daily_video_metrics['views'].pct_change() * 100

# Calculate daily growth rate in revenue
revenue_metrics['revenue_growth'] = revenue_metrics['estimatedRevenue'].pct_change() * 100

# View the new columns in the data
print(daily_video_metrics[['day', 'views', 'views_growth']].tail())
print(revenue_metrics[['day', 'estimatedRevenue', 'revenue_growth']].tail())

```

Rolling Average for Views
We can create a rolling average to smooth out fluctuations in the data. Let's calculate the 7-day rolling average for video views.


```{python}


# Calculate rolling average for views over 7 days
daily_video_metrics['views_7_day_avg'] = daily_video_metrics['views'].rolling(window=7).mean()

# View the new columns in the data
print(daily_video_metrics[['day', 'views', 'views_7_day_avg']].tail())
```

Engagement Metrics (Likes, Comments, Shares)
We'll calculate engagement metrics such as the ratio of likes to views, comments to views, and share counts.

```{python}
# Calculate engagement rate (likes/views)
daily_annotation_metrics['engagement_rate'] = daily_annotation_metrics['likes'] / daily_video_metrics['views']

# Calculate comment engagement rate # Display first rows of data
print("Daily Video Metrics:")
daily_video_metrics.tail()

print("Comments:")
comments.tail()

print("Province Metrics:")
province_metrics.head()

print("Daily Annotation Metrics:")
daily_annotation_metrics.head()

print("Traffic Source Metrics:")
traffic_source_metrics.head()

print("Revenue Metrics:")
revenue_metrics.tail()

print("Ad Type Metrics:")
ad_type_metrics.tail()

print("Sharing Metrics:")
sharing_metrics.tail()

print("Daily Country Specific Metrics:")
daily_country_specific_metrics.tail()(comments/views)
comments['comment_engagement'] = comments.groupby('video_id')['comment'].transform('count') / daily_video_metrics.groupby('video_id')['views'].transform('sum')

# Display the new engagement rates
print(daily_annotation_metrics[['day', 'engagement_rate']].head())
print(comments[['video_id', 'comment_engagement']].head())

```

## 3. Data Analysis and Visualization




```{python}
plt.figure(figsize=(10, 8))
corr = daily_video_metrics[['views', 'estimatedMinutesWatched', 'averageViewDuration', 'subscribersGained']].corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix for Video Metrics")
plt.show()



```


```{python}
sns.pairplot(daily_video_metrics[['views', 'estimatedMinutesWatched', 'averageViewDuration', 'subscribersGained']])
plt.suptitle("Pairwise Relationships Among Video Metrics", y=1.02)
plt.show()


```


```{python}
# Ensure 'day' is datetime in revenue_metrics
revenue_metrics['day'] = pd.to_datetime(revenue_metrics['day'])

fig = px.line(
    revenue_metrics,
    x='day',
    y='estimatedRevenue',
    title='Estimated Revenue Over Time',
    labels={'day': 'Date', 'estimatedRevenue': 'Estimated Revenue'}
)
fig.show()

```
```{python}
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Select features for clustering
features = daily_video_metrics[['views', 'estimatedMinutesWatched', 'subscribersGained']]
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

# Apply K-Means clustering (adjust n_clusters as needed)
kmeans = KMeans(n_clusters=3, random_state=42)
daily_video_metrics['cluster'] = kmeans.fit_predict(scaled_features)

# Visualize clusters using a scatter plot (example: views vs. estimatedMinutesWatched)
plt.figure(figsize=(10, 6))
sns.scatterplot(
    data=daily_video_metrics, 
    x='views', 
    y='estimatedMinutesWatched', 
    hue='cluster', 
    palette='viridis',
    s=100
)
plt.title("K-Means Clustering of Videos")
plt.xlabel("Views")
plt.ylabel("Estimated Minutes Watched")
plt.show()

```
```{python}

import pycountry
import pandas as pd

def convert_to_iso3(two_letter_code):
    try:
        country = pycountry.countries.get(alpha_2=two_letter_code)
        return country.alpha_3 if country else None
    except:
        return None

# Assuming your 'country' column has two-letter codes
daily_country_specific_metrics['iso_alpha'] = daily_country_specific_metrics['country'].apply(convert_to_iso3)


import plotly.express as px

fig = px.choropleth(
    daily_country_specific_metrics,
    locations='iso_alpha',     # Now using the new column with ISO-3 codes
    locationmode='ISO-3',       
    color='views',
    color_continuous_scale='Viridis',
    title='Views by Country'
)
fig.show()


```

```{python}

# Install prophet if needed: pip install prophet
from prophet import Prophet
import pandas as pd
import matplotlib.pyplot as plt

# Prepare your data
df_views = daily_video_metrics[['day', 'views']].copy()
df_views.columns = ['ds', 'y']  # Prophet requires ds (date) and y (value)

# Initialize and fit the model
model = Prophet()
model.fit(df_views)

# Create a dataframe for future dates
future = model.make_future_dataframe(periods=50)  # Forecast 30 days into the future
forecast = model.predict(future)

# Plot forecast
fig = model.plot(forecast)
plt.title('Forecasted Daily Views')
plt.show()


```


```{python}
from wordcloud import WordCloud

# Concatenate comments for each sentiment category
positive_comments = " ".join(comments[comments['sentiment_label'] == 'POSITIVE']['comment'])
negative_comments = " ".join(comments[comments['sentiment_label'] == 'NEGATIVE']['comment'])

# Create and display a word cloud for positive comments
wordcloud_pos = WordCloud(width=800, height=400, background_color='white').generate(positive_comments)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_pos, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Positive Comments')
plt.show()

# Create and display a word cloud for negative comments
wordcloud_neg = WordCloud(width=800, height=400, background_color='white').generate(negative_comments)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_neg, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Negative Comments')
plt.show()

```

```{python}
# %%
import streamlit as st
import plotly.express as px

st.title("YouTube Data Dashboard")

# Date range filter
start_date = st.sidebar.date_input("Start Date", value=pd.to_datetime("2021-01-01"))
end_date = st.sidebar.date_input("End Date", value=pd.to_datetime("today"))

# Filter daily_video_metrics
filtered_data = daily_video_metrics[(pd.to_datetime(daily_video_metrics['day']) >= pd.to_datetime(start_date)) &
                                    (pd.to_datetime(daily_video_metrics['day']) <= pd.to_datetime(end_date))]

# Line chart for views
fig = px.line(filtered_data, x='day', y='views', title="Views Over Time")
st.plotly_chart(fig)

# Other visualizations can be added similarly...

```